# Multi-Armed Bandit Algorithms

This project implements and compares popular Multi-Armed Bandit algorithms:

- ðŸŽ¯ **Epsilon-Greedy**
- ðŸ“ˆ **UCB1 (Upper Confidence Bound)**
- ðŸŽ² **Thompson Sampling**

The goal is to simulate a scenario where we repeatedly choose between different options ("arms") to maximize the total reward, just like picking ads, treatments, or online recommendations.

---
