# Multi-Armed Bandit Algorithms

This project implements and compares popular Multi-Armed Bandit algorithms:

- ğŸ¯ **Epsilon-Greedy**
- ğŸ“ˆ **UCB1 (Upper Confidence Bound)**
- ğŸ² **Thompson Sampling**

The goal is to simulate a scenario where we repeatedly choose between different options ("arms") to maximize the total reward, just like picking ads, treatments, or online recommendations.

---

## ğŸ” Purpose

This project serves as a practical and intuitive demonstration of key strategies for solving the **exploration vs exploitation** dilemma using Bandit algorithms. It's meant for:

- ğŸ“˜ Learning and experimenting with Bandit strategies
- ğŸ“Š Comparing different algorithms' performance
- ğŸ§  Understanding real-world decision making with uncertainty
- ğŸ§‘â€ğŸ’» Building a solid portfolio project

---

## ğŸš€ Quick Start

### ğŸ”§ Installation
```bash
pip install numpy matplotlib
```

### â–¶ï¸ Run the Project
```bash
python bandit_simulation.py
```
Youâ€™ll see a graph comparing the cumulative rewards of each strategy.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ bandit_simulation.py      # Main code implementing all algorithms
â”œâ”€â”€ README.md                 # Overview & instructions
â””â”€â”€ explanation.md            # In-depth explanation of algorithms and results
```

---

## ğŸ“ˆ Algorithms Overview

- **Epsilon-Greedy**: Explores randomly with probability Îµ, otherwise picks best-known option.
- **UCB1**: Picks based on current average + confidence interval (more balanced).
- **Thompson Sampling**: Samples from a probability distribution and picks the most likely winner.

---

## ğŸ“Š Result Preview
The script visualizes cumulative rewards from all 3 strategies over 1000 iterations. Helps see which learns fastest.

---

## ğŸ“˜ License
This project is open-source and free to use under the MIT license.

---

## ğŸ™‹â€â™‚ï¸ Author
**Vinit Singh Pathir**  
[GitHub](https://github.com/Vinit-4689) | [LinkedIn](www.linkedin.com/in/vinit-singh-cse)

---

>**â€œThe more you explore, the better you exploit.â€**

